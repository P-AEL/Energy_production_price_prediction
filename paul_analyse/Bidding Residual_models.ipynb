{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import logging\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bbidding = pd.read_csv('bidding_training.csv')\n",
    "df_bidding_imbalance_price = pd.read_csv('bidding_training_predictions_imbalance_price.csv')\n",
    "df_bidding_imbalance_price = df_bidding_imbalance_price[[\"timestamp_utc\",\"imbalance_price_predictions\"]]\n",
    "df_bidding_imbalance_price.timestamp_utc = pd.to_datetime(df_bidding_imbalance_price.timestamp_utc)\n",
    "df_bidding_day_ahead_price = pd.read_csv('bidding_training_predictions_day_ahead_price.csv')\n",
    "df_bidding_day_ahead_price = df_bidding_day_ahead_price[[\"timestamp_utc\",\"day_ahead_price_predictions\"]]\n",
    "df_bidding_day_ahead_price.timestamp_utc = pd.to_datetime(df_bidding_day_ahead_price.timestamp_utc)\n",
    "df_bbidding[\"day_ahead_price\"] = df_bbidding[\"price_x\"].rename(\"day_ahead_price\")\n",
    "df_bbidding[\"market_price\"] = df_bbidding[\"price_y\"].rename(\"market_price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bbidding.timestamp_utc = pd.to_datetime(df_bbidding.timestamp_utc)\n",
    "df_bbidding = df_bbidding.merge(df_bidding_imbalance_price, on=\"timestamp_utc\")\n",
    "df_bbidding = df_bbidding.merge(df_bidding_day_ahead_price, on=\"timestamp_utc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define the revenue function (objective function)\n",
    "def revenue(zb, DAP, Target_MW, imbalance_price):\n",
    "    return zb * DAP + (Target_MW - zb) * (imbalance_price - 0.07 * (Target_MW - zb))\n",
    "\n",
    "# Negative revenue function (for minimization)\n",
    "def negative_revenue(zb, DAP, Target_MW, imbalance_price):\n",
    "    return -revenue(zb, DAP, Target_MW, imbalance_price)\n",
    "\n",
    "# Optimization function to compute the optimal bidding value for each row\n",
    "def optimize_bidding(row):\n",
    "    # Extract the values from the row\n",
    "    DAP = row['day_ahead_price']\n",
    "    Target_MW = row['Target_MW']\n",
    "    imbalance_price = row['imbalance_price']\n",
    "    \n",
    "    # Initial guess for zb (midpoint between 0 and Target_MW)\n",
    "    initial_zb = Target_MW / 2\n",
    "    \n",
    "    # Bounds for zb (as per KKT conditions)\n",
    "    bounds = [(0, 1800)]\n",
    "    \n",
    "    # Perform the optimization\n",
    "    result = minimize(negative_revenue, initial_zb, args=(DAP, Target_MW, imbalance_price), bounds=bounds)\n",
    "    \n",
    "    # Optimal trade value (zb)\n",
    "    return result.x[0]\n",
    "\n",
    "# Apply the optimization to each row and replace column '5' with the optimized trade value\n",
    "df_bbidding['optimized_trade'] = df_bbidding.apply(optimize_bidding, axis=1)\n",
    "\n",
    "# Now calculate the revenue using the optimized trade values\n",
    "df_bbidding['revenue_optimal'] = df_bbidding['day_ahead_price'] * df_bbidding['optimized_trade'] + \\\n",
    "                         (df_bbidding['Target_MW'] - df_bbidding['optimized_trade']) * \\\n",
    "                         (df_bbidding['imbalance_price'] - 0.07 * (df_bbidding['Target_MW'] - df_bbidding['optimized_trade']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56634.21239495645"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bbidding.revenue_optimal.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define the revenue function (objective function)\n",
    "def revenue(zb, DAP, Target_MW, imbalance_price):\n",
    "    return zb * DAP + (Target_MW - zb) * (imbalance_price - 0.07 * (Target_MW - zb))\n",
    "\n",
    "# Negative revenue function (for minimization)\n",
    "def negative_revenue(zb, DAP, Target_MW, imbalance_price):\n",
    "    return -revenue(zb, DAP, Target_MW, imbalance_price)\n",
    "\n",
    "# Optimization function to compute the optimal bidding value for each row\n",
    "def optimize_bidding(row):\n",
    "    # Extract the values from the row\n",
    "    DAP = row['day_ahead_price_predictions']\n",
    "    Target_MW = row['1']\n",
    "    imbalance_price = row['imbalance_price_predictions']\n",
    "    \n",
    "    # Initial guess for zb (midpoint between 0 and Target_MW)\n",
    "    initial_zb = Target_MW / 2\n",
    "    \n",
    "    # Bounds for zb (as per KKT conditions)\n",
    "    bounds = [(0, 1800)]\n",
    "    \n",
    "    # Perform the optimization\n",
    "    result = minimize(negative_revenue, initial_zb, args=(DAP, Target_MW, imbalance_price), bounds=bounds)\n",
    "    \n",
    "    # Optimal trade value (zb)\n",
    "    return result.x[0]\n",
    "\n",
    "# Apply the optimization to each row and replace column '5' with the optimized trade value\n",
    "df_bbidding['optimized_trade_prediction'] = df_bbidding.apply(optimize_bidding, axis=1)\n",
    "\n",
    "# Now calculate the revenue using the optimized trade values\n",
    "df_bbidding['revenue_normal_prediction'] = df_bbidding['day_ahead_price'] * df_bbidding['optimized_trade_prediction'] + \\\n",
    "                         (df_bbidding['Target_MW'] - df_bbidding['optimized_trade_prediction']) * \\\n",
    "                         (df_bbidding['imbalance_price'] - 0.07 * (df_bbidding['Target_MW'] - df_bbidding['optimized_trade_prediction']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33860.38059863207"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bbidding.revenue_normal_prediction.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bbidding[\"trade_residuals\"] = df_bbidding[\"optimized_trade\"] - df_bbidding[\"optimized_trade_prediction\"]\n",
    "y = df_bbidding[\"trade_residuals\"]\n",
    "X = df_bbidding[[\"day_ahead_price_predictions\",\"imbalance_price_predictions\",\"optimized_trade_prediction\",\"cos_hour\",\"cos_day\",\"1\",\"9\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import numpy as np\n",
    "\n",
    "# # Convert data to tensors\n",
    "# class TradeDataset(Dataset):\n",
    "#     def __init__(self, X, y):\n",
    "#         self.X = torch.FloatTensor(X)\n",
    "#         self.y = torch.FloatTensor(y.values.reshape(-1, 1))\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.X)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.X[idx], self.y[idx]\n",
    "\n",
    "# # Define the MLP model\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, input_size):\n",
    "#         super(MLP, self).__init__()\n",
    "#         self.layers = nn.Sequential(\n",
    "#             nn.Linear(input_size, 128),\n",
    "#             nn.SELU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(128, 256),\n",
    "#             nn.SELU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(256, 64),\n",
    "#             nn.SELU(),\n",
    "#             nn.Linear(64, 1)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         return self.layers(x)\n",
    "\n",
    "# # Prepare the data\n",
    "# scaler1 = StandardScaler()\n",
    "# X_scaled = scaler1.fit_transform(X)\n",
    "\n",
    "# # Split the data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create data loaders\n",
    "# train_dataset = TradeDataset(X_train, y_train)\n",
    "# test_dataset = TradeDataset(X_test, y_test)\n",
    "\n",
    "# batch_size = 512\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# # Initialize model, loss function, and optimizer\n",
    "# model = MLP(input_size=X.shape[1])\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Training loop\n",
    "# num_epochs = 1000\n",
    "# train_losses = []\n",
    "# test_losses = []\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     train_loss = 0\n",
    "#     for batch_X, batch_y in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(batch_X)\n",
    "#         loss = criterion(outputs, batch_y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_loss += loss.item()\n",
    "    \n",
    "#     # Calculate average training loss\n",
    "#     train_loss = train_loss / len(train_loader)\n",
    "#     train_losses.append(train_loss)\n",
    "    \n",
    "#     # Validation\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch_X, batch_y in test_loader:\n",
    "#             outputs = model(batch_X)\n",
    "#             loss = criterion(outputs, batch_y)\n",
    "#             test_loss += loss.item()\n",
    "    \n",
    "#     test_loss = test_loss / len(test_loader)\n",
    "#     test_losses.append(test_loss)\n",
    "    \n",
    "#     if (epoch + 1) % 10 == 0:\n",
    "#         print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
    "\n",
    "# # Plot training curves\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(train_losses, label='Training Loss')\n",
    "# plt.plot(test_losses, label='Test Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Training and Test Loss over Time')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# # Evaluate final model\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     X_test_tensor = torch.FloatTensor(X_test)\n",
    "#     y_pred = model(X_test_tensor).numpy()\n",
    "    \n",
    "# # Calculate metrics\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# print(f'\\nFinal Results:')\n",
    "# print(f'MSE: {mse:.4f}')\n",
    "# print(f'RMSE: {np.sqrt(mse):.4f}')\n",
    "# print(f'R2 Score: {r2:.4f}')\n",
    "\n",
    "# # Plot predictions vs actual values\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "# plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "# plt.xlabel('Actual Values')\n",
    "# plt.ylabel('Predicted Values')\n",
    "# plt.title('Predicted vs Actual Values')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'trade_residuals_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# def objective(trial, X_train, X_test, y_train, y_test):\n",
    "#     \"\"\"\n",
    "#     Objective function for Optuna optimization\n",
    "#     \"\"\"\n",
    "#     params = {\n",
    "#         'objective': 'regression',\n",
    "#         'metric': 'l2',\n",
    "#         'boosting_type': 'gbdt',\n",
    "#         'verbose': -1,\n",
    "        \n",
    "#         # Parameters to optimize\n",
    "#         'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "#         'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "#         'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "#         'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "#         'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "#         'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 1.0),\n",
    "#         'min_child_weight': trial.suggest_float('min_child_weight', 0.001, 10.0),\n",
    "#         'bagging_freq': trial.suggest_int('bagging_freq', 1, 10)\n",
    "#     }\n",
    "\n",
    "#     # Create datasets\n",
    "#     train_data = lgb.Dataset(X_train, y_train)\n",
    "#     valid_data = lgb.Dataset(X_test, y_test, reference=train_data)\n",
    "\n",
    "#     # Train model with callbacks\n",
    "#     model = lgb.train(\n",
    "#         params,\n",
    "#         train_data,\n",
    "#         valid_sets=[valid_data],\n",
    "#         num_boost_round=1000,\n",
    "#         callbacks=[\n",
    "#             lgb.early_stopping(stopping_rounds=20),\n",
    "#             lgb.log_evaluation(period=100)\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     # Predict and calculate loss\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     loss = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "#     # Log progress\n",
    "#     logging.info(f\"Trial {trial.number} - MSE: {loss:.4f}\")\n",
    "\n",
    "#     return loss\n",
    "\n",
    "# # Split data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create and optimize study\n",
    "# study = optuna.create_study(direction=\"minimize\")\n",
    "# study.optimize(\n",
    "#     lambda trial: objective(trial, X_train, X_test, y_train, y_test),\n",
    "#     n_trials=20\n",
    "# )\n",
    "\n",
    "# # Get best parameters and train final model\n",
    "# best_params = study.best_params\n",
    "# best_params['objective'] = 'regression'\n",
    "# best_params['metric'] = 'l2'\n",
    "# best_params['boosting_type'] = 'gbdt'\n",
    "# best_params['verbose'] = -1\n",
    "\n",
    "# print(\"\\nBest parameters:\")\n",
    "# for key, value in best_params.items():\n",
    "#     print(f\"    {key}: {value}\")\n",
    "\n",
    "# # Train final model with best parameters\n",
    "# train_data = lgb.Dataset(X_train, y_train)\n",
    "# valid_data = lgb.Dataset(X_test, y_test, reference=train_data)\n",
    "\n",
    "# final_model = lgb.train(\n",
    "#     best_params,\n",
    "#     train_data,\n",
    "#     valid_sets=[valid_data],\n",
    "#     num_boost_round=1000,\n",
    "#     callbacks=[\n",
    "#         lgb.early_stopping(stopping_rounds=20),\n",
    "#         lgb.log_evaluation(period=100)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Make predictions and evaluate\n",
    "# y_pred = final_model.predict(X_test)\n",
    "# final_mse = mean_squared_error(y_test, y_pred)\n",
    "# print(f\"\\nFinal MSE: {final_mse:.4f}\")\n",
    "\n",
    "# # Calculate feature importance\n",
    "# importance = final_model.feature_importance(importance_type='gain')\n",
    "# feature_names = final_model.feature_name()\n",
    "\n",
    "# feature_importance = pd.DataFrame({\n",
    "#     'Feature': feature_names,\n",
    "#     'Importance': importance\n",
    "# }).sort_values('Importance', ascending=False)\n",
    "\n",
    "# print(\"\\nTop 20 Most Important Features:\")\n",
    "# print(feature_importance.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[788]\tvalid_0's l2: 117729\n",
      "Mean Squared Error: 117729.0149270751\n",
      "Model saved as lightgbm_model.joblib\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create dataset for LightGBM\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Define parameters\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'l2',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 237,\n",
    "    'learning_rate': 0.017557987676990648,\n",
    "    'max_depth': 13,\n",
    "    'min_child_samples': 5,\n",
    "    'subsample': 0.9285594584090331,\n",
    "    'colsample_bytree': 0.9163968558386822,\n",
    "    'reg_alpha': 0.17689372588678054,\n",
    "    'reg_lambda': 0.7918520649641817,\n",
    "    'min_split_gain': 0.3740091850958246,\n",
    "    'min_child_weight': 8.558715178577113,\n",
    "    'bagging_freq': 6,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# Train the model with early stopping\n",
    "gbm = lgb.train(params,\n",
    "                train_set=lgb_train,\n",
    "                num_boost_round=1000,  # Set high to enable early stopping\n",
    "                valid_sets=[lgb_eval],  # Use test set for early stopping validation\n",
    "                callbacks=[lgb.early_stopping(stopping_rounds=10)])  # Early stopping callback\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Save the model using joblib\n",
    "joblib.dump(gbm, 'lightgbm_model.joblib')\n",
    "print(\"Model saved as lightgbm_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = gbm.predict(X)\n",
    "df_bbidding[\"trade_residuals_predictions\"] = predictions1 + df_bbidding[\"optimized_trade_prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bbidding['revenue_normal_prediction'] = df_bbidding['day_ahead_price'] * df_bbidding['trade_residuals_predictions'] + \\\n",
    "                         (df_bbidding['Target_MW'] - df_bbidding['trade_residuals_predictions']) * \\\n",
    "                         (df_bbidding['imbalance_price'] - 0.07 * (df_bbidding['Target_MW'] - df_bbidding['trade_residuals_predictions']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48261.6212235938"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bbidding.revenue_normal_prediction.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.1):\n",
    "        super(LSTMPredictor, self).__init__()\n",
    "        \n",
    "        # Parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define the LSTM layer(s)\n",
    "        self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, \n",
    "                            num_layers=self.num_layers, batch_first=True, dropout=self.dropout)\n",
    "        \n",
    "        # Fully connected layer to map LSTM output to the target size\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states for LSTM\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)  # Hidden state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)  # Cell state\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # We only need the output\n",
    "        \n",
    "        # Get the last output (many-to-one), out[:, -1, :] gives the last time step\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Pass the output through a fully connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "\n",
    "path_df = os.path.abspath(os.path.join(current_dir, '..', 'basic_files'))\n",
    "df_total_solar = pd.read_csv(os.path.join(path_df, 'solar_total_production.csv'))\n",
    "df_total_solar.generation_mw = df_total_solar.generation_mw *0.5\n",
    "df_total_wind = pd.read_csv(os.path.join(path_df, 'wind_total_production.csv'))\n",
    "df_total_wind.generation_mw = df_total_wind.generation_mw *0.5 - df_total_wind.boa\n",
    "df_imbalance_price = pd.read_csv(os.path.join(path_df, 'imbalance_price.csv'))\n",
    "df_day_ahead_price = pd.read_csv(os.path.join(path_df, 'day_ahead_price.csv'))\n",
    "df_market_price = pd.read_csv(os.path.join(path_df, 'market_index.csv'))\n",
    "\n",
    "# Get the path to the 'logs' directory in the parent directory\n",
    "path = os.path.abspath(os.path.join(current_dir, '..', 'logs'))\n",
    "files = os.listdir(path)\n",
    "txt_files = [file for file in files if file.endswith('.txt')]\n",
    "data = []\n",
    "for file in txt_files:\n",
    "    with open(os.path.join(path, file), 'r') as f:\n",
    "        try:\n",
    "            json_data = json.load(f)\n",
    "            data.append(json_data)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to decode JSON from file: {file}\")\n",
    "date_name = []\n",
    "for i in range(len(data)):\n",
    "    date_name.append(data[i][\"prediction_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrahiere Daten\n",
    "dataframe_list = []\n",
    "\n",
    "for entry in data:\n",
    "    prediction_date = entry['prediction_date']\n",
    "    \n",
    "    # Iteriere durch jedes 'submission' Element\n",
    "    for submission in entry['solution']['submission']:\n",
    "        timestamp = submission['timestamp']\n",
    "        probabilistic_forecast = submission['probabilistic_forecast']\n",
    "        \n",
    "        # Extrahiere die Werte von 'probabilistic_forecast' und füge sie der Liste hinzu\n",
    "        row = {\n",
    "            'prediction_date': prediction_date,\n",
    "            'timestamp': timestamp,\n",
    "            '1': probabilistic_forecast.get('10', None),\n",
    "            '2': probabilistic_forecast.get('20', None),\n",
    "            '3': probabilistic_forecast.get('30', None),\n",
    "            '4': probabilistic_forecast.get('40', None),\n",
    "            '5': probabilistic_forecast.get('50', None),\n",
    "            '6': probabilistic_forecast.get('60', None),\n",
    "            '7': probabilistic_forecast.get('70', None),\n",
    "            '8': probabilistic_forecast.get('80', None),\n",
    "            '9': probabilistic_forecast.get('90', None)\n",
    "        }\n",
    "        dataframe_list.append(row)\n",
    "\n",
    "# Erstelle DataFrame\n",
    "df_api_new = pd.DataFrame(dataframe_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api_new = df_api_new.groupby(\"timestamp\").last().reset_index()\n",
    "df_api_new.timestamp = pd.to_datetime(df_api_new.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a continuous time series from the minimum to maximum timestamp at 30-minute intervals\n",
    "full_timestamp_range = pd.date_range(start=df_api_new['timestamp'].min(), end=df_api_new['timestamp'].max(), freq='30min')\n",
    "# Reindex the dataframe using the full range of timestamps\n",
    "df_api_new_1 = df_api_new.set_index('timestamp').reindex(full_timestamp_range, method=None)\n",
    "df_api_new_1 = df_api_new_1.reset_index().rename(columns={'index': 'timestamp'})\n",
    "# Create the 'prediction_date' column based on the timestamp\n",
    "df_api_new_1['prediction_date'] = df_api_new_1['timestamp'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day_ahead_price.timestamp_utc = pd.to_datetime(df_day_ahead_price.timestamp_utc)\n",
    "df_market_price.timestamp_utc = pd.to_datetime(df_market_price.timestamp_utc)\n",
    "df_imbalance_price.timestamp_utc = pd.to_datetime(df_imbalance_price.timestamp_utc)\n",
    "df_api_new_merged = pd.merge(df_api_new_1,df_day_ahead_price, left_on='timestamp', right_on='timestamp_utc', how='left')\n",
    "df_api_new_merged = pd.merge(df_api_new_merged,df_market_price, left_on='timestamp', right_on='timestamp_utc', how='left')\n",
    "df_api_new_merged = pd.merge(df_api_new_merged,df_imbalance_price, left_on='timestamp', right_on='timestamp_utc', how='left')\n",
    "df_api_new_merged[\"day_ahead_price\"] = df_api_new_merged[\"price_x\"].rename(\"day_ahead_price\")\n",
    "df_api_new_merged[\"market_price\"] = df_api_new_merged[\"price_y\"].rename(\"market_price\")\n",
    "df_api_new_merged[\"settlement_period\"] = df_api_new_merged[\"settlement_period_x\"].rename(\"settlement_period\")\n",
    "df_api_new_merged[\"cos_hour\"] = np.cos(2*np.pi*df_api_new_merged[\"timestamp\"].dt.hour/24)\n",
    "df_api_new_merged[\"cos_day\"] = np.cos(2*np.pi*df_api_new_merged[\"timestamp\"].dt.day/7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api_new_merged1 = df_api_new_merged[[\"timestamp_utc\",\"market_price\",\"day_ahead_price\",\"volume\",\"settlement_period\",\"cos_hour\",\"cos_day\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"imbalance_price\"]].copy()\n",
    "df_api_new_merged1.loc[:,\"market_price_lag96h\"] = df_api_new_merged1[\"market_price\"].shift(192)\n",
    "df_api_new_merged1.loc[:,\"imbalance_price_lag96h\"] = df_api_new_merged1[\"imbalance_price\"].shift(192)\n",
    "df_api_new_merged1.loc[:,\"day_ahead_price_lag1week\"] = df_api_new_merged1[\"day_ahead_price\"].shift(192)\n",
    "df_api_new_merged1.loc[:,\"volume_lag96h\"] = df_api_new_merged1[\"volume\"].shift(96)\n",
    "df_api_new_merged1.dropna(inplace=True)\n",
    "df_api_new_merged1 = df_api_new_merged1.groupby(\"timestamp_utc\").last().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_total = pd.read_csv('D:/Users/paulh/Desktop/Domäneprojekt2/Energy_production_price_prediction/basic_files/solar_total_production.csv')\n",
    "wind_total = pd.read_csv('D:/Users/paulh/Desktop/Domäneprojekt2/Energy_production_price_prediction/basic_files/wind_total_production.csv')\n",
    "solar_total.generation_mw = solar_total.generation_mw * 0.5\n",
    "wind_total.generation_mw = wind_total.generation_mw * 0.5 - wind_total.boa\n",
    "solar_total.timestamp_utc = pd.to_datetime(solar_total.timestamp_utc)\n",
    "wind_total.timestamp_utc = pd.to_datetime(wind_total.timestamp_utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api_new_merged2 = pd.merge(df_api_new_merged1,solar_total, on=\"timestamp_utc\", how=\"inner\")\n",
    "df_api_new_merged2 = pd.merge(df_api_new_merged2,wind_total, on=\"timestamp_utc\", how=\"inner\")\n",
    "df_api_new_merged2 = df_api_new_merged2.rename(columns={\n",
    "    \"generation_mw_x\": \"generation_solar\",\n",
    "    \"generation_mw_y\": \"generation_wind\"\n",
    "})\n",
    "df_api_new_merged2 = df_api_new_merged2.groupby(\"timestamp_utc\").last().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paulh\\anaconda3\\envs\\HEFTcom24\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "df_api_new_merged2_X = df_api_new_merged2[[\"market_price_lag96h\",\"imbalance_price_lag96h\",\"day_ahead_price_lag1week\",\"volume_lag96h\",\n",
    "                    \"cos_hour\",\"cos_day\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]].copy()\n",
    "\n",
    "scaler_path = \"LSTM_imbalance_scaler.pkl\"\n",
    "# Laden des StandardScalers aus der Datei\n",
    "with open(scaler_path, 'rb') as file:\n",
    "    scaler = pickle.load(file)\n",
    "\n",
    "# Skalieren der Daten\n",
    "df_api_new_merged2_X_scaled = scaler.transform(df_api_new_merged2_X)\n",
    "\n",
    "# Konvertieren der Daten in PyTorch-Tensoren\n",
    "X_test = torch.tensor(df_api_new_merged2_X_scaled, dtype=torch.float32)\n",
    "X_test = X_test.unsqueeze(1)  # Adds a sequence length dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulh\\AppData\\Local\\Temp\\ipykernel_6324\\1576695596.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_imbalance.load_state_dict(torch.load(\"LSTM_imbalance_price.pth\"))\n"
     ]
    }
   ],
   "source": [
    "input_size = 15  # Number of features\n",
    "hidden_size = 64              # Number of LSTM units\n",
    "num_layers = 3                 # Number of LSTM layers\n",
    "output_size = 1                # Always 9 for 9 quantiles\n",
    "dropout = 0.1  \n",
    "model_imbalance = LSTMPredictor(input_size, hidden_size, num_layers, output_size, dropout=dropout)\n",
    "model_imbalance.load_state_dict(torch.load(\"LSTM_imbalance_price.pth\"))\n",
    "# Modell in den Evaluierungsmodus versetzen\n",
    "model_imbalance.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model_imbalance(X_test)\n",
    "predictions = predictions.numpy()\n",
    "df_api_new_merged2[\"imvalance_price_predictions\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulh\\AppData\\Local\\Temp\\ipykernel_6324\\823899303.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_imbalance.load_state_dict(torch.load(\"LSTM_day_ahead_price.pth\"))\n"
     ]
    }
   ],
   "source": [
    "input_size = 15  # Number of features\n",
    "hidden_size = 64              # Number of LSTM units\n",
    "num_layers = 3                 # Number of LSTM layers\n",
    "output_size = 1                # Always 9 for 9 quantiles\n",
    "dropout = 0.1  \n",
    "model_imbalance = LSTMPredictor(input_size, hidden_size, num_layers, output_size, dropout=dropout)\n",
    "model_imbalance.load_state_dict(torch.load(\"LSTM_day_ahead_price.pth\"))\n",
    "# Modell in den Evaluierungsmodus versetzen\n",
    "model_imbalance.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model_imbalance(X_test)\n",
    "predictions = predictions.numpy()\n",
    "df_api_new_merged2[\"day_ahead_price_predictions\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api_new_merged2[\"Total_MW\"] = df_api_new_merged2[\"generation_solar\"] + df_api_new_merged2[\"generation_wind\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define the revenue function (objective function)\n",
    "def revenue(zb, DAP, Target_MW, imbalance_price):\n",
    "    return zb * DAP + (Target_MW - zb) * (imbalance_price - 0.07 * (Target_MW - zb))\n",
    "\n",
    "# Negative revenue function (for minimization)\n",
    "def negative_revenue(zb, DAP, Target_MW, imbalance_price):\n",
    "    return -revenue(zb, DAP, Target_MW, imbalance_price)\n",
    "\n",
    "# Optimization function to compute the optimal bidding value for each row\n",
    "def optimize_bidding(row):\n",
    "    # Extract the values from the row\n",
    "    DAP = row['day_ahead_price_predictions']\n",
    "    Target_MW = row['1']\n",
    "    imbalance_price = row['imvalance_price_predictions']\n",
    "    \n",
    "    # Initial guess for zb (midpoint between 0 and Target_MW)\n",
    "    initial_zb = Target_MW / 2\n",
    "    \n",
    "    # Bounds for zb (as per KKT conditions)\n",
    "    bounds = [(0, 1800)]\n",
    "    \n",
    "    # Perform the optimization\n",
    "    result = minimize(negative_revenue, initial_zb, args=(DAP, Target_MW, imbalance_price), bounds=bounds)\n",
    "    \n",
    "    # Optimal trade value (zb)\n",
    "    return result.x[0]\n",
    "\n",
    "# Apply the optimization to each row and replace column '5' with the optimized trade value\n",
    "df_api_new_merged2['optimized_trade'] = df_api_new_merged2.apply(optimize_bidding, axis=1)\n",
    "\n",
    "# Now calculate the revenue using the optimized trade values\n",
    "df_api_new_merged2['revenue_normal'] = df_api_new_merged2['day_ahead_price'] * df_api_new_merged2['optimized_trade'] + \\\n",
    "                         (df_api_new_merged2['Total_MW'] - df_api_new_merged2['optimized_trade']) * \\\n",
    "                         (df_api_new_merged2['imbalance_price'] - 0.07 * (df_api_new_merged2['Total_MW'] - df_api_new_merged2['optimized_trade']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21886.00915783455"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_api_new_merged2.revenue_normal.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api_new_merged2\n",
    "X = df_api_new_merged2[[\"day_ahead_price_predictions\",\"imvalance_price_predictions\",\"optimized_trade\",\"cos_hour\",\"cos_day\",\"1\",\"9\"]]\n",
    "predictions = gbm.predict(X)\n",
    "df_api_new_merged2[\"newly_optimized_trade\"] = predictions + df_api_new_merged2[\"optimized_trade\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api_new_merged2['revenue_normal_new'] = df_api_new_merged2['day_ahead_price'] * df_api_new_merged2['newly_optimized_trade'] + \\\n",
    "                         (df_api_new_merged2['Total_MW'] - df_api_new_merged2['newly_optimized_trade']) * \\\n",
    "                         (df_api_new_merged2['imbalance_price'] - 0.07 * (df_api_new_merged2['Total_MW'] - df_api_new_merged2['newly_optimized_trade']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19745.46583094493"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_api_new_merged2.revenue_normal_new.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_scaled = scaler1.transform(X.values)\n",
    "# X_scaled = torch.tensor(X_scaled, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     predictions_mlp = model(X_scaled)\n",
    "# predictions_mlp = predictions_mlp.numpy()\n",
    "# predictions_mlp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (predictions_mlp + df_api_new_merged2[[\"optimized_trade\"]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_api_new_merged2[\"optimized_trade_prediction_mlp\"] = predictions_mlp + df_api_new_merged2[[\"optimized_trade\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'optimized_trade_prediction_mlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\paulh\\anaconda3\\envs\\HEFTcom24\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'optimized_trade_prediction_mlp'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_api_new_merged2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrevenue_normal_new\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_api_new_merged2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday_ahead_price\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[43mdf_api_new_merged2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimized_trade_prediction_mlp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m      2\u001b[0m                          (df_api_new_merged2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal_MW\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m df_api_new_merged2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimized_trade_prediction_mlp\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m*\u001b[39m \\\n\u001b[0;32m      3\u001b[0m                          (df_api_new_merged2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimbalance_price\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.07\u001b[39m \u001b[38;5;241m*\u001b[39m (df_api_new_merged2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal_MW\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m df_api_new_merged2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimized_trade_prediction_mlp\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\paulh\\anaconda3\\envs\\HEFTcom24\\Lib\\site-packages\\pandas\\core\\frame.py:3896\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3896\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3898\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\paulh\\anaconda3\\envs\\HEFTcom24\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'optimized_trade_prediction_mlp'"
     ]
    }
   ],
   "source": [
    "df_api_new_merged2['revenue_normal_new'] = df_api_new_merged2['day_ahead_price'] * df_api_new_merged2['optimized_trade_prediction_mlp'] + \\\n",
    "                         (df_api_new_merged2['Total_MW'] - df_api_new_merged2['optimized_trade_prediction_mlp']) * \\\n",
    "                         (df_api_new_merged2['imbalance_price'] - 0.07 * (df_api_new_merged2['Total_MW'] - df_api_new_merged2['optimized_trade_prediction_mlp']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2047.165563438268"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_api_new_merged2.revenue_normal_new.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HEFTcom24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
